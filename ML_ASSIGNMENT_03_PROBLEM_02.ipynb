{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CopotronicRifat/CS-5783-Machine-Learning-Assignments/blob/main/ML_ASSIGNMENT_03_PROBLEM_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UnsbqGRX7wMr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JnkLBLeF7ddp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c0e4ab-51e6-42aa-eafa-215c79680268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P4Nvsp8B8QGl"
      },
      "outputs": [],
      "source": [
        "# LeNet model\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(6, (5, 5), activation='relu', strides = (1,1), input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides = (2, 2)))\n",
        "model.add(layers.Conv2D(16, (5, 5), strides = (1,1), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2), strides = (2, 2)))\n",
        "model.add(layers.Conv2D(120, (5, 5), activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6NgRlUXfBPRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3448960b-ce17-494a-e134-be6fda3d4446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 6)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 1, 1, 120)         48120     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50,992\n",
            "Trainable params: 50,992\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_Qw015LQBmG9"
      },
      "outputs": [],
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(84, activation='relu'))\n",
        "model.add(layers.Dense(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n6ODPDUcNaAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "203e8749-e7e0-4a3c-dea5-00292c63605c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 6)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 1, 1, 120)         48120     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 120)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YoC4uyoPNcWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e77dfd4-8286-4b7d-8c6e-6cb12fd86feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 11s 5ms/step - loss: 1.6419 - accuracy: 0.3966 - val_loss: 1.3882 - val_accuracy: 0.4953\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3601 - accuracy: 0.5091 - val_loss: 1.2846 - val_accuracy: 0.5340\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2528 - accuracy: 0.5530 - val_loss: 1.2664 - val_accuracy: 0.5414\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1801 - accuracy: 0.5810 - val_loss: 1.2570 - val_accuracy: 0.5512\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.1287 - accuracy: 0.5985 - val_loss: 1.1874 - val_accuracy: 0.5785\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0785 - accuracy: 0.6178 - val_loss: 1.1666 - val_accuracy: 0.5830\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0410 - accuracy: 0.6325 - val_loss: 1.1234 - val_accuracy: 0.6044\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.0021 - accuracy: 0.6446 - val_loss: 1.1423 - val_accuracy: 0.6009\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9736 - accuracy: 0.6544 - val_loss: 1.1316 - val_accuracy: 0.6037\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.9395 - accuracy: 0.6652 - val_loss: 1.1160 - val_accuracy: 0.6098\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.9091 - accuracy: 0.6779 - val_loss: 1.1283 - val_accuracy: 0.6058\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8868 - accuracy: 0.6846 - val_loss: 1.0977 - val_accuracy: 0.6204\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8681 - accuracy: 0.6925 - val_loss: 1.1128 - val_accuracy: 0.6114\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8427 - accuracy: 0.7001 - val_loss: 1.0934 - val_accuracy: 0.6197\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8289 - accuracy: 0.7047 - val_loss: 1.1798 - val_accuracy: 0.6048\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8097 - accuracy: 0.7107 - val_loss: 1.1388 - val_accuracy: 0.6119\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7912 - accuracy: 0.7198 - val_loss: 1.1736 - val_accuracy: 0.6042\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7713 - accuracy: 0.7241 - val_loss: 1.1738 - val_accuracy: 0.6133\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7565 - accuracy: 0.7335 - val_loss: 1.1811 - val_accuracy: 0.6133\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7416 - accuracy: 0.7360 - val_loss: 1.1962 - val_accuracy: 0.6111\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7274 - accuracy: 0.7406 - val_loss: 1.2267 - val_accuracy: 0.6092\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7149 - accuracy: 0.7449 - val_loss: 1.1978 - val_accuracy: 0.6186\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6997 - accuracy: 0.7502 - val_loss: 1.2194 - val_accuracy: 0.6079\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6862 - accuracy: 0.7542 - val_loss: 1.2418 - val_accuracy: 0.5999\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6736 - accuracy: 0.7582 - val_loss: 1.2535 - val_accuracy: 0.6117\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=25, \n",
        "                    validation_data=(test_images, test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Xf7zKZqRsiut"
      },
      "outputs": [],
      "source": [
        "learningrates_list = [0.001,0.01,0.1,1.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cXjoUfYGtyGe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8ccdde7-91ca-4e01-83f6-70a4a4f2c793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.3578 - val_loss: 6.8033\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 6.5785 - val_loss: 6.1457\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 5.9474 - val_loss: 5.8716\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 5.4746 - val_loss: 5.6772\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 5.1496 - val_loss: 5.2453\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 4.8590 - val_loss: 5.1915\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 4.5927 - val_loss: 5.1658\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 4.4008 - val_loss: 5.1484\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 4.1320 - val_loss: 5.1857\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 3.9513 - val_loss: 5.0247\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 3.7415 - val_loss: 4.7946\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 3.5475 - val_loss: 4.9829\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 3.3739 - val_loss: 5.4719\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 3.2348 - val_loss: 4.9349\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 3.0997 - val_loss: 5.2019\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.9250 - val_loss: 5.4910\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.8361 - val_loss: 5.1376\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.7076 - val_loss: 5.1591\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.5572 - val_loss: 5.4505\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.4576 - val_loss: 5.1122\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3748 - val_loss: 5.2621\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.2804 - val_loss: 5.4017\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.1739 - val_loss: 5.2569\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0843 - val_loss: 5.8221\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0263 - val_loss: 5.4818\n",
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 7.5226 - val_loss: 6.9001\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 6.9340 - val_loss: 6.7159\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 6.6468 - val_loss: 6.1166\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 6.4147 - val_loss: 6.2400\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 6.1804 - val_loss: 6.3744\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 6.1083 - val_loss: 6.4131\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 5.9060 - val_loss: 6.1711\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 7.3695 - val_loss: 8.3600\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2681 - val_loss: 8.2647\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2685 - val_loss: 8.2501\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2603 - val_loss: 8.2858\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2590 - val_loss: 8.2875\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2559 - val_loss: 8.2506\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2527 - val_loss: 8.2512\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2523 - val_loss: 8.2664\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2539 - val_loss: 8.2577\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2535 - val_loss: 8.2504\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2532 - val_loss: 8.2500\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2534 - val_loss: 8.2503\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2543 - val_loss: 8.2500\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2535 - val_loss: 8.2508\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2522 - val_loss: 8.2624\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2543 - val_loss: 8.2528\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2524 - val_loss: 8.2573\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.2515 - val_loss: 8.2540\n",
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 8.2832 - val_loss: 8.3435\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.2729 - val_loss: 8.2514\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.2725 - val_loss: 8.2612\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.2731 - val_loss: 8.2668\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2767 - val_loss: 8.2500\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2738 - val_loss: 8.2502\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2723 - val_loss: 8.3311\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 8.2760 - val_loss: 8.2867\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2759 - val_loss: 8.2532\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.2758 - val_loss: 8.2511\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.2725 - val_loss: 8.2500\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2773 - val_loss: 8.2507\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2743 - val_loss: 8.2569\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.2745 - val_loss: 8.2500\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2686 - val_loss: 8.2722\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.2732 - val_loss: 8.2540\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2713 - val_loss: 8.2504\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2718 - val_loss: 8.2503\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2808 - val_loss: 8.2768\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2691 - val_loss: 8.2500\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2715 - val_loss: 8.2599\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2780 - val_loss: 8.2501\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2742 - val_loss: 8.2507\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.2805 - val_loss: 8.2570\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.2776 - val_loss: 8.2505\n",
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.4571 - val_loss: 8.6863\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.4063 - val_loss: 8.3669\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.4337 - val_loss: 8.3919\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.4555 - val_loss: 8.2573\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.4599 - val_loss: 8.3988\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.4524 - val_loss: 8.3516\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.4637 - val_loss: 8.6952\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.4580 - val_loss: 8.3042\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.4416 - val_loss: 8.2669\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.4539 - val_loss: 8.2546\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.4656 - val_loss: 8.3244\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.4475 - val_loss: 8.3560\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.4835 - val_loss: 8.5573\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.4418 - val_loss: 8.4607\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.4589 - val_loss: 8.3924\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.4755 - val_loss: 8.3132\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.4381 - val_loss: 8.4268\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.4392 - val_loss: 8.6562\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.4523 - val_loss: 8.3247\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 8.4308 - val_loss: 8.3951\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.4629 - val_loss: 8.4217\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.4687 - val_loss: 8.2986\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.4910 - val_loss: 8.2626\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.4648 - val_loss: 8.4592\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.4491 - val_loss: 8.3205\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "\n",
        "for i in range(len(learningrates_list)):\n",
        "    optimizer = keras.optimizers.Adam(lr = learningrates_list[i])\n",
        "    model.compile(loss='mse', optimizer=optimizer)\n",
        "    history = model.fit(train_images, train_labels, epochs=25, validation_data=(test_images, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2.1**\n",
        "\n",
        "**Effect of Learning rate:**\n",
        "\n",
        "As the learning rate increases the loss also increases. Hence accuracy increases.\n",
        "\n",
        "Also, I have got the best performance when learning rate is 0.001, compared to 0.01, 0.1, 1.0.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5VGxpNQ6CD0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batchsize_list = [8, 16, 32, 64, 128]"
      ],
      "metadata": {
        "id": "NpBGSXwwCsrq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "for i in range(len(learningrates_list)):\n",
        "    print('Result for ' + str(batchsize_list[i]))\n",
        "    optimizer = keras.optimizers.Adam(lr = 0.001)\n",
        "    model.compile(loss='mse', optimizer=optimizer)\n",
        "    history = model.fit(train_images, train_labels, epochs = 25, batch_size = batchsize_list[i])"
      ],
      "metadata": {
        "id": "GEjLxSlcC5QV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d4bc27-dc20-4f3f-e969-0d10400e7cb8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for 8\n",
            "Epoch 1/25\n",
            "6250/6250 [==============================] - 19s 3ms/step - loss: 8.2567\n",
            "Epoch 2/25\n",
            "6250/6250 [==============================] - 19s 3ms/step - loss: 8.2506\n",
            "Epoch 3/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2509\n",
            "Epoch 4/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2507\n",
            "Epoch 5/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2506\n",
            "Epoch 6/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2506\n",
            "Epoch 7/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2506\n",
            "Epoch 8/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2508\n",
            "Epoch 9/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2508\n",
            "Epoch 10/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2509\n",
            "Epoch 11/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2506\n",
            "Epoch 12/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2508\n",
            "Epoch 13/25\n",
            "6250/6250 [==============================] - 19s 3ms/step - loss: 8.2505\n",
            "Epoch 14/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2506\n",
            "Epoch 15/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2505\n",
            "Epoch 16/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2502\n",
            "Epoch 17/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2507\n",
            "Epoch 18/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2506\n",
            "Epoch 19/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2509\n",
            "Epoch 20/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2504\n",
            "Epoch 21/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2506\n",
            "Epoch 22/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2506\n",
            "Epoch 23/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2504\n",
            "Epoch 24/25\n",
            "6250/6250 [==============================] - 19s 3ms/step - loss: 8.2507\n",
            "Epoch 25/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2508\n",
            "Result for 16\n",
            "Epoch 1/25\n",
            "3125/3125 [==============================] - 10s 3ms/step - loss: 8.2504\n",
            "Epoch 2/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2507\n",
            "Epoch 3/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2505\n",
            "Epoch 4/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2506\n",
            "Epoch 5/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2504\n",
            "Epoch 6/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2506\n",
            "Epoch 7/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2506\n",
            "Epoch 8/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2505\n",
            "Epoch 9/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2505\n",
            "Epoch 10/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2504\n",
            "Epoch 11/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2506\n",
            "Epoch 12/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2506\n",
            "Epoch 13/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2505\n",
            "Epoch 14/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2499\n",
            "Epoch 15/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2506\n",
            "Epoch 16/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2505\n",
            "Epoch 17/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2506\n",
            "Epoch 18/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2501\n",
            "Epoch 19/25\n",
            "3125/3125 [==============================] - 10s 3ms/step - loss: 8.2504\n",
            "Epoch 20/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2502\n",
            "Epoch 21/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2499\n",
            "Epoch 22/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2507\n",
            "Epoch 23/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2506\n",
            "Epoch 24/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2504\n",
            "Epoch 25/25\n",
            "3125/3125 [==============================] - 9s 3ms/step - loss: 8.2506\n",
            "Result for 32\n",
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 8.2503\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2505\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2503\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2504\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2500\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2505\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2504\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2504\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2504\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2502\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2504\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2505\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2503\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2504\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2501\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2505\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2504\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 8.2503\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2504\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2505\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2504\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2504\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2504\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2503\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 8.2505\n",
            "Result for 64\n",
            "Epoch 1/25\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 8.2503\n",
            "Epoch 2/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 3/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 4/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 5/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 6/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 7/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2501\n",
            "Epoch 8/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 9/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 10/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2501\n",
            "Epoch 11/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 12/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 13/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 14/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 15/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2502\n",
            "Epoch 16/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2504\n",
            "Epoch 17/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2504\n",
            "Epoch 18/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 19/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 20/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 21/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 22/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 23/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2503\n",
            "Epoch 24/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2505\n",
            "Epoch 25/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 8.2502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "for i in range(len(learningrates_list)):\n",
        "    for j in range(len(batchsize_list)):\n",
        "\n",
        "        print('For learning rate' + str(learningrates_list[i]) + 'and batch size ' + str(batchsize_list[i]) + ':')\n",
        "        optimizer = keras.optimizers.Adam(lr = learningrates_list[i])\n",
        "        model.compile(loss='mse', optimizer=optimizer)\n",
        "        history = model.fit(train_images, train_labels, epochs = 25, batch_size = batchsize_list[i])"
      ],
      "metadata": {
        "id": "66VMwQg9GVlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "990eee76-105b-49b1-e781-85a64e796ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For learning rate0.001and batch size 8:\n",
            "Epoch 1/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2508\n",
            "Epoch 2/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2503\n",
            "Epoch 3/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2508\n",
            "Epoch 4/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2503\n",
            "Epoch 5/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2505\n",
            "Epoch 6/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2505\n",
            "Epoch 7/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2502\n",
            "Epoch 8/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2506\n",
            "Epoch 9/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2505\n",
            "Epoch 10/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2507\n",
            "Epoch 11/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2508\n",
            "Epoch 12/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2508\n",
            "Epoch 13/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2509\n",
            "Epoch 14/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2506\n",
            "Epoch 15/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2508\n",
            "Epoch 16/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2502\n",
            "Epoch 17/25\n",
            "6250/6250 [==============================] - 19s 3ms/step - loss: 8.2508\n",
            "Epoch 18/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2506\n",
            "Epoch 19/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2508\n",
            "Epoch 20/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2504\n",
            "Epoch 21/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2507\n",
            "Epoch 22/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2508\n",
            "Epoch 23/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2506\n",
            "Epoch 24/25\n",
            "6250/6250 [==============================] - 18s 3ms/step - loss: 8.2507\n",
            "Epoch 25/25\n",
            "5165/6250 [=======================>......] - ETA: 3s - loss: 8.2474"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2.3**\n",
        "\n",
        "We tried different hyperparameter, such as batch size and learning rate."
      ],
      "metadata": {
        "id": "wT6ILx7YGjqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Equivalent Feed Forward Network\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(6, activation='relu'))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(120, activation='relu'))\n",
        "model.add(layers.Dense(84, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='relu'))"
      ],
      "metadata": {
        "id": "CKDGsCJjFvRA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=25, \n",
        "                    validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "id": "X-qBk2i2GQJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec3016e8-9a86-4845-950f-7cddb4ac674c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3026 - accuracy: 0.0999 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2.3**\n",
        "\n",
        "I have compared different learning rates as well as different batch size. From the result, we can see that learning rate works well when the value of the learning rate is small. And, the batch size does not have any significant effect as far as the performance is concern."
      ],
      "metadata": {
        "id": "Z1ICDKxfmFsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(train_images, train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "c9VqCO56bE6y",
        "outputId": "e704e049-c8c0-4b8c-a11e-f896fb6af997"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3026 - accuracy: 0.1000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmYCsXUQlRle",
        "outputId": "eab8567b-6e81-40a7-d041-327330568388"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 3072)              0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 6)                 18438     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                112       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 120)               2040      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,604\n",
            "Trainable params: 31,604\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2.4.a**\n",
        "\n",
        "From the result, we can see that, the equvalent feed forward network is very poor based on its performance compared to LeNet.\n",
        "\n",
        "\n",
        "**Problem 2.4.b**\n",
        "\n",
        "From the model summary, we get-\n",
        "\n",
        "i. Number of parameters in LeNet is 62,006.\n",
        "ii. Number of parameters in Feed Forward Net is 31,604.\n",
        "\n",
        "As you can see, Feed Forward Net is almost half the parameters in comparison with LeNet, but it is NOT worth it, as the result of Feed Forward Net is very poor compared to LeNet."
      ],
      "metadata": {
        "id": "A-siZadibVZj"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOX/vQp243AQNSEJCfboUgl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}